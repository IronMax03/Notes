\documentclass[5pt]{article}
\usepackage{multicol,multirow}
\usepackage{graphicx} % Required for inserting images
\usepackage[margin=0.75cm]{geometry}
\usepackage{xcolor}
\usepackage{amsmath,esint}
\usepackage{mathtools}
\usepackage{relsize}
\usepackage{mathtools}
\usepackage{nccmath}
\usepackage[inline]{enumitem}
\usepackage{algpseudocode}

\usepackage{empheq}
\usepackage{amsfonts}

\usepackage{tkz-euclide}
\usepackage{tikz}

\definecolor{LightGray}{gray}{0.9}

\usepackage{minted}

\newenvironment{amatrix}[1]{%
  \left[\begin{array}{@{}*{#1}{c}|c@{}}
}{%
  \end{array}\right]
}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%

\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}

\newcommand{\tr}[3]{
\begin{tikzpicture}[scale=0.40]
    \coordinate [] (A) at (-1.5cm,-1.cm);
    \coordinate [] (C) at (1.5cm,-1.0cm);
    \coordinate [] (B) at (1.5cm,1.0cm);
    \draw (A) -- node[above] {$a$} (B) -- node[right] {$b$} (C) -- node[below] {$c$} (A);
    \draw (1.25cm,-1.0cm) rectangle (1.5cm,-0.75cm);
\end{tikzpicture}
}


\begin{document}

\begin{center}
     \Large{\textbf{Linear Algebra from Elementary to Advanced}}\\
     \footnotesize{University: Johns Hopkins University}\hfill\footnotesize{\textcopyright Maximilien Notz \the\year{}}
     \noindent\rule{20.2cm}{0.4pt}
\end{center}


\begin{multicols}{2}
\setcounter{secnumdepth}{0}

\section{Linear Systems and Matrix Equations}
\subsection{Definition}
\begin{tabular}{ll}
    Linear Functions        & All terms are of  degree 0 or 1. \\
    Consistent              & lin. systems is consistent if either 1 or $\infty$ \\
                            & solutions exist else inconsistent.\\
\small{Row-Equivalence}     & Two matrce are row-equivalent if there is\\
                            & a sequence of \textbf{EROS} that transforms one\\
                            & into the other.\\
    Pivot Position          & is a position in a matrix that corresponds\\
                            & to a leading 1 in the matrix reduced\\
                            & echelon form.\\
    Pivot Columns           & is a culumn of $A$ that contain a \textbf{pivot}.\\
    Leading Entry           & of a row in a matrix is the leftmost nonzero\\
                            & entry in the row.\\
    Basic Variables         & in a system of linear equations are the\\
                            & variables that correspond to pivot columns\\
                            & of the matrix for the system.\\                    
    Free Variables          & are the variables that do not correspond to\\
                            & pivot columns.\\
    Column Vector           & is a matrix with only one column.\\
    Lin. Combination        & \footnotesize{$\begin{pmatrix}A_1\\ \vdots\\ A_n\end{pmatrix}=C_1 + ... + C_m$}\\
    Span                    & $\text{span}(\vec{v})$ is the set of all scalar multiples of\\
                            & $\vec{v}$, where $\vec{c}\neq\vec{0}$. \\
    Identity Matrix         & \footnotesize{$\begin{bmatrix}
                                  1 & 0 & ... & 0 \\
                                  0 & 1 & ... & 0 \\
                                  \vdots & \vdots & \ddots & \vdots \\
                                  0 & 0 & ... & 1 \\
                                \end{bmatrix}_{n\times n}$}\\
   Homogeneous              & A homogeneous system of linear equations\\
                            & can be written in the form $A\vec{x}=\vec{0}$.\\
   Trivial Sol.             & The trivial solution to a homogeneous\\
                            & system is $\vec{x}=\vec{0}$. Any other solutions are\\
                            & called nontrivial solutions.\\
\small{Linearly Dependent}  & A set of vectors $\{\vec{v}_1, \vec{v}_2, ..., \vec{v}_n\}\in\mathbb{R}^n$ is\\
                            & \textbf{linearly dependent} if there exist weights\\
                            & such that $\vec{c}_1\vec{v}_1 + \vec{c}_2\vec{v}_2 + ... + \vec{c}_p\vec{v}_p=\vec{0}$.\\
\small{Surjective}\footnotesize{(onto)}                  & For a \textbf{linear transformation} $T$:\\
                            & for every vector $\vec{w}\in W$, there exist a\\
                            & vector $\vec{v}\in V$ such that $T(v)=W$.\\
\small{Injective}\footnotesize{(one-to-one)}                   & For a \textbf{linear transformation} $T$:\\
                            & with $V\rightarrow W$, if $T(\vec{v}_1)=T(\vec{v}_2)$, then $\vec{v}_1=\vec{v}_2$\\
                            & meaning that each vector in $V$ maps to a\\
                            & unique vector in $W$.\\
Standard Vector             & $\vec{e}_j=$\footnotesize{$\begin{pmatrix}0\\ \vdots\\ 1\\ \vdots\\ 0\end{pmatrix}$} \footnotesize{The Standard vectors of $\mathbb{R}^n$ are $\{\vec{e}_1, ..., \vec{e}_n\}$}\\
\end{tabular}
\begin{tabular}{ll}
Linearly Independent  & $\{\vec{v}_1, \vec{v}_2, ..., \vec{v}_n\}$ is \textbf{linearly independent}\\
                      & if the equation $x_1\vec{v}_1 + x_2\vec{v}_2 + ... + x_n\vec{v}_n$\\
Linear Transformation & is a transformation $T$ that satisfies\\
                      & both the following conditions:\\
                      & $\bullet$ $T(\vec{u}+\vec{v}) = T(\vec{u}) + T(\vec{v})$\\
                      & $\bullet$ $T(c\vec{u})=cT(\vec{u})$\\
\end{tabular}

\subsection{Theorems}
\newtheorem{theorem}{Theorem}
\newtheorem{properties}{Properties}

\begin{properties}
  Let $\vec{u},\vec{v}, \vec{w}\in \mathbb{R}^n$ and $c,d\in\mathbb{R}$ then:
  \begin{itemize*}
    \item $\vec{u}+\vec{v}=\vec{v}+\vec{u}$
    \item $\vec{u}+\vec{0}=\vec{u}$
    \item $\vec{u}+(-\vec{u})=\vec{0}$
    \item $c(\vec{u}+\vec{v})=c\vec{u}+c\vec{v}$
    \item $(c+d)\vec{w}=c\vec{w}+d\vec{w}$
    \item $c(d\vec{u})=(cd)\vec{u}$
  \end{itemize*}
\end{properties}

\begin{theorem}[Uniqueness of Reduced Echelon Form]
  Every matrix is row equivalent to a unique row echelon form.
\end{theorem}

\begin{theorem}[Existence of Solutions]
  A system of linear equations is consistent if and only if the last (rightmost) column of its augmented
matrix is not a pivot column.
\end{theorem}

\begin{theorem}[Matrix Eq., Vector Equ., and Lin. Systems]
  If $A$ is a $m\times n$ matrix  with columns $\vec{a}_1, ..., \vec{a}_n$, $\vec{b}\in\mathbb{R}^n$ 
  then the matrix equation $A\vec{x}=\vec{b}$ has same solution set as vector equation $x_1\vec{a}_1 + ... + x_n\vec{a}_n=\vec{b}$  
  wich is the same solution set to system of linear equation with augmented matrix 
  $\left[A|\vec{b}\right]=\left[\vec{a}_1, ..., \vec{a}_n|\vec{b}\right]$ 
\end{theorem}

\begin{theorem}[Existence of Solutions]
  Let $A_{n\times m}$ the following are equivalent:
  \begin{itemize*}
    \item $\forall\vec{b}\in\mathbb{R}^n, A\vec{x}=\vec{b}$ has a solution.
    \item each $\vec{b}$ is the linear combination of the columns of $A$.
    \item Columns of $A$ span $\mathbb{R}^n$.
    \item $A$ has a pivot in every row.
  \end{itemize*}
\end{theorem}

\begin{theorem}
  If $A_{m\times n},\vec{u},\vec{v}\in\mathbb{R}^n$ $c\in\mathbb{R}$, where $A\vec{u}$ is defined\\
  \begin{itemize*}
    \item $A(\vec{u}+\vec{v})=A\vec{u}+A\vec{v}$
    \item $A(c\vec{u})=c(A\vec{u})$
  \end{itemize*}
\end{theorem}

\begin{theorem}[Nontrivial Sol. of Homogeneous Equ.]
  The homogeneous equation $A\vec{x}=0$ has a nontrivial solution if and only if the equation has at least
  one free variable.
\end{theorem}

\begin{theorem}[Solution Sets with Free Variables]
  Suppose the equation $A\vec{x}=\vec{b}$ is consistent. Let $p$ be a particular solution to the equation $A\vec{x}=\vec{b}$ 
  Then the solution set of $A\vec{x}=\vec{b}$ is the set of all vectors of the form $\vec{w}=\vec{p}+\vec{v}_h$,
  where $v_h$ is the solution of the homogeneous equation $A\vec{x}=\vec{0}$.
\end{theorem}

\begin{theorem}[Sets of Two or More Vectors]
  The set of  $s=\{\vec{v}_1,\vec{v}_2, ... ,\vec{v}_p\}$ of 2 or more vectors 
  is linearly dependent if and only if at least 1 vector is a linear combination 
  of the other.
\end{theorem}

\begin{theorem}[Sets with More Vectors than Entries]
  If $p>n$, then the set $\{\vec{v}_1, \vec{v}_2, ..., \vec{v}_n\}\in\mathbb{R}$ 
  must be linearly dependent.
\end{theorem}

\begin{theorem}[Sets Containing the Zero Vector]
  Any set of vectors that contains the zero vector is linearly dependent.
\end{theorem}

\begin{theorem}[The Standard Matrix of a Lin. Trans.]
  Let $T:\mathbb{R}^n\rightarrow\mathbb{R}^m$ be a linear transformation. Then $T$ 
  is injective if and only if $T(\vec{x})=\vec{0}$ has only the trivial solution.
\end{theorem}

\subsection{Coefficient Matrix Example}
\begin{equation}
\centering
\left\{\begin{split}
A_1x_1+A_2x_2+A_3x_3=\alpha \\
B_1x_1+B_2x_2+B_3x_3=\beta \\
\end{split}\right.
\Leftrightarrow
\begin{bmatrix}
    A_1 & A_2 & A_3\\
    B_1 & B_2 & B_3\\
\end{bmatrix}
\end{equation}

\subsection{Augmented Matrix Example}
\begin{equation}
\left\{\begin{split}
A_1x_1+A_2x_2+A_3x_3=\alpha \\
B_1x_1+B_2x_2+B_3x_3=\beta \\
\end{split}\right.
\Leftrightarrow
\begin{amatrix}{3}
    A_1 & A_2 & A_3 & \alpha \\
    B_1 & B_2 & B_3 & \beta \\
 \end{amatrix}
\end{equation}

\subsection{Elementary Row Operations (EROS)}
1. \textbf{[Replacement]} Replace one row by sum of itself. \\
2. \textbf{[Interchange]} Swap position of 2 rows. \\
3. \textbf{[Scaling]} Multiply all entries in row by non-zero constant. \\


\subsection{Echelon Form (ef)}
1. All non-zero rows are above any rows of all-zero. \\
2. Each leading entry of a row is in a column to the right of the roe above it. \\
3. All entries in a column below a leading entry are 0. \\


\subsection{Reduced Row Echelon Form (rref)}
1. As to be in echelon form. \\
2. Leading entry in each row is 1. \\
3. Each leading 1 is the only non-zero entry in its column. \\

\subsection{Vector Operations}
$\begin{pmatrix}A_1\\ A_2\end{pmatrix}+\begin{pmatrix}B_1\\ B_2\end{pmatrix}=\begin{pmatrix}A_1+B_1\\ A_2+B_2\end{pmatrix}\;\;\;\;$
$C\begin{pmatrix}A_1\\ A_2\end{pmatrix}=\begin{pmatrix}CA_1\\ CA_2\end{pmatrix}, C\in\mathbb{R}$

\subsection{Matrix \& Vector Multiplication}
Let $A_{m\times n}, \vec{x}\in\mathbb{R}^n$. $A\vec{x}=\left[a_1, ...,a_n\right]\begin{pmatrix}x_1\\ \vdots\\ x_n\end{pmatrix}=a_1x_1 + ... + a_nx_n$


\section{Matrix Algebra, Determinants, \& Eigenvectors}

\section{Orthogonality and Diagonalization}
\subsection{Definition}
\begin{tabular}{ll}
  Inner Product       & $\vec{v}\cdot\vec{u}=\vec{v}^T\vec{u}=u_1v_1 + ... + u_nv_n$\\
                      & \footnotesize{(Also called dot product or scalar product)}\\
  Length of $\vec{x}$ & $||\vec{x}||=\sqrt{\vec{x}\cdot\vec{x}}=\sqrt{x^2_1+...+x^2_n}$\\
                      & \footnotesize{(Also called Norm or Magnitude)}\\
  Unit vectore        & A vector with $||\vec{x}||=1$ \\
  Normalization       & The formula $\vec{u}=\frac{\vec{x}}{||\vec{x}||}$ creat a unit vector \\
                      & in the same direction as $\vec{x}$. \\
  Distance            &  $dist(\color{blue}\vec{u}\color{black},\color{red}\vec{v}\color{black})=\color{blue}\vec{u}\color{black}-\color{red}\vec{v}$      
                        \begin{tikzpicture}[scale=0.50,baseline={([yshift={-\ht\strutbox-12}]current bounding box.north)}]
                          \draw[line width=1pt,black](2,0)--(2,1) node[anchor=south west]{};
                          \draw[line width=1pt,blue,-stealth](0,0)--(2,1) node[anchor=south west]{};
                          \draw[line width=1pt,red,-stealth](0,0)--(2,0) node[anchor=south]{}; 
                          \draw [line width=0.5pt]
                            (2,0.5) coordinate (d) node[right] {\footnotesize$dist(\vec{u},\vec{v})$}
                            (2,0) coordinate (v) node[right] {}
                            (2,1) coordinate (u) node[right] {};
                        \end{tikzpicture}\\
Orthogonality         & $\vec{u}\cdot \vec{v} = 0$
                      \begin{tikzpicture}[scale=0.5,baseline={([yshift={-\ht\strutbox-10}]current bounding box.north)}]
                        \draw[black,step=0.3, line width=1pt] (0,0) grid (0.3,0.3);
                        \draw[->,line width=1pt] (0,0)--(1,0) node[above]{\footnotesize$\vec{u}$};
                        \draw[->,line width=1pt,] (0,0)--(0,1) node[right]{\footnotesize$\vec{v}$};
                      \end{tikzpicture}\\
Orthogonal Set        & A set  of vectors $\{\vec{u}_1, ..., \vec{u}_p\}\in\mathbb{R}^n$ such that\\ 
                      & each distinct vectors are orthogonal.\\
Orthogonal Basis      & For a subspace $W$ of $R^n$ is a basis that is\\
                      & also an orthogonal set.\\
Orthonormal set       & is a set  of  orthogonal unit vectors.\\
Orthogonal matrix     & Is a squared matrix whose columns are\\
                      & orthonormal.\\
Normal equation       & $A^TA\vec{x}=A^T\vec{b}$\\
Least-Squares Error   & $||\vec{b}-A\hat{x}||$\\
Symetric              & $A_{m\times n}$ is symetric if $A=A^T$\\
Spectrum              & Set of eigenvalues of $A_{n\times n}\{\lambda_1,...,\lambda_n\}$ is\\
                      & called the \textbf{spectrum} of $A$.\\
Quadratic Form        & A \textbf{quadratic form} on $\mathbb{R}^n$ is a function $Q$ \\
                      & from $\mathbb{R}^n\rightarrow\mathbb{R}$ of the form $Q(\vec{x})=\vec{x}^TA\vec{x}$,\\
                      & where $A$ is a symetric matrix.
\end{tabular}
\begin{tabular}{ll}
Orthogonal Complements  & Let $W$ be a subspace of $\mathbb{R}^n$.\\
                        & The orthogonal complement of $W$ is: \\
                        & $W^\perp=\{\vec{x}\in\mathbb{R}^n|\vec{x}\cdot\vec{w}=\vec{0}, \forall\vec{w}\in W\}$\\
Orthogonal Projection   & of $\vec{y}\in\mathbb{R}^n$ onto subspace $W$ of $\mathbb{R}^n$ is\\
                        & the vector $\hat{y}$ in $W$ such that $\vec{y} -\hat{y}$ \\
                        & is in $W^\perp$. (Noted $\text{proj}_W(\vec{y})$)\\
Least-Squares Problem   & is to find $\vec{x}$ that makes $||\vec{b}-A\vec{x}||$ \\
                        & $=\text{dist}(\vec{b},A\vec{x})$ as small as possible. 
\end{tabular}
\begin{tabular}{ll}
  Orthogonally Diagonalizable Matrix & is a square matrix for which\\
                                     & there exist an orthogonal \\
                                     & matrix $P$ and a diagonal \\
                                     & matrix $D$ such that \\
                                     & $A=PDP^{-1}=PDP^T$.
\end{tabular}

\subsection{Theorems}
\begin{properties}
  Let $\vec{u},\vec{v},\vec{w} \in \mathbb{R}^n$ and $c\in \mathbb{R}^n$ then:\\
  \begin{itemize*}
    \item $\vec{u}\cdot\vec{v}=\vec{v}\cdot\vec{u}$ 
    \item $(\vec{u}+\vec{v})\cdot \vec{w}=\vec{u}\cdot\vec{w}+\vec{v}\cdot \vec{w}$
    \item $(c\vec{u})\cdot \vec{v}=c(\vec{u}\cdot \vec{v})=(c\vec{v})\cdot \vec{u}$
    \item $\vec{u}\cdot \vec{u}=u_1^2+...+u^2_n\geq 0$
  \end{itemize*}
\end{properties}

\begin{theorem}[Fundamental Subspaces Theorem]
  Let $A_{M\times N}$ then:
  \begin{itemize*}
    \item $(row(A))^\perp=nul(A)$
    \item $(COl(A))^\perp=nul(a^\perp)$
  \end{itemize*}
\end{theorem}

\begin{theorem}
  If $S=\{\vec{u}_1,...,\vec{u}_p\}$ is an \textbf{orthogonal set} of non-zero vectors in $\mathbb{R}$ then $S$ is a \textbf{linearly independant} set. 
\end{theorem}

\begin{theorem}
  let $\{\vec{u}_1,...,\vec{u}_p\}$ be an \textbf{orthogonal basis} for $w\subset \mathbb{R}^n$. 
  Let $y\in W$. Then $\vec{y}=C_1\vec{u}_1 + ... + C_p\vec{u}_p$, $C_j=\frac{\vec{y}\cdot\vec{u}_i}{\vec{u}_i\cdot\vec{u}_i}$
\end{theorem}

\begin{theorem}
  Let $u=(\vec{u}_1 \vec{u}_2 \vec{u}_3)$, where $\{\vec{u}_1, \vec{u}_2, \vec{u}_3\}$ is orthonormal set.
  $u^T\cdot u= I$
\end{theorem}

\begin{theorem}
  Let $u$ be an \textbf{orthogonal matrix}, Let $\vec{x},\vec{y}\in\mathbb{R}$:
  \begin{itemize*}
    \item $|u\vec{x}|=|\vec{x}|\:$
    \item $(u\vec{x})\cdot(u\vec{y})=\vec{x}\cdot\vec{y}$
  \end{itemize*}
\end{theorem}

\begin{theorem}[Orthogonal Decomposition Theorem]
  Let $W$ be a subspace with an orthogonal basis $\{\vec{u}_1,...,\vec{u}_p\}$. let $\vec{y}\in\mathbb{R}^n$ then: 
  $\hat{y}=\left(\frac{\vec{y}\cdot\vec{u}_1}{\vec{u}_1\cdot\vec{u}_1}\right)\vec{u}_1 + ... + \left(\frac{\vec{y}\cdot\vec{u}_p}{\vec{u}_p\cdot\vec{u}_p}\right)\vec{u}_p$
 \begin{tikzpicture}[scale=0.50, baseline={([yshift={-\ht\strutbox}]current bounding box.north)}]
    \draw[line width=1pt,black,-stealth](4,0)--(4,1) node[anchor=south west]{};
    \draw[line width=1pt,blue,-stealth](0,0)--(4,1) node[anchor=south west]{};
    \draw[line width=1pt,red,-stealth](0,0)--(4,0) node[anchor=south]{}; 
    \draw [line width=0.5pt]
      (2,1) coordinate (v) node[right] {\footnotesize{$\color{blue}\vec{}{y}$}}
      (4,1) coordinate (d) node[right] {\footnotesize{$\hat{y}^\perp=\vec{y}-\hat{y}$}}
      (4,0) coordinate (u) node[right] {\footnotesize{$\color{red}\hat{y}$}};
  \end{tikzpicture}
\end{theorem}

\begin{theorem}[Best Approximation Theorem]
  Let $W$ be a subspace of $\mathbb{R}$. Let $\vec{y}\in\mathbb{R}^n$ and $\hat{y}=\text{proj}_W(\vec{}y)$. 
  Then $\hat{y}$ is closest point to $\vec{y}$ in $W$. That is $\forall\vec{v}\ne\vec{y}\;\; ||\vec{y}-\hat{y}||<||\vec{y}-\vec{v}||$  
\end{theorem}

\begin{theorem}[Orth. Proj. with Orthonormal Bases]
  Let $W$ subspace in $\mathbb{R}^n$. 
  Let $\{\vec{u}_1, ..., \vec{u}_p\}$ be an orthonormal basis of  $W$.
  $\text{proj}_W(\vec{y})=(\vec{y}\cdot\vec{u}_1)\vec{u}_1 + ... + (\vec{y}\cdot\vec{u}_p)\vec{u}_p$
  Let $U=(\vec{u}_1\; \vec{u}_2\; ...\; \vec{u}_p)$. Then $\text{proj}_W(\vec{y})=UU^T\vec{y}$
\end{theorem}

\begin{theorem}[Normal Equations and Least-Squares Sol.]
  The set of least squares solutions of $A\vec{x}=\vec{b}$ coincide the non-empty solutions of the Normal equation.
\end{theorem}

\begin{theorem}[Criteria for a Unique Least-Squares Sol.]
  Let $A_{M\times N}$ then the following are equivalent: 
  \begin{itemize*}
    \item $A\vec{x}=\vec{b}$ has unique least-square solution $\forall\vec{b}\in\mathbb{R}^n$.
    \item Columns of $A$ are linearly independant.
    \item The matrix $A^TA$ is invertible.
  \end{itemize*}
  When the statements are true, the least squares solution is given by $\hat{x}=(A^TA)^{-1}A^T\vec{b}$
\end{theorem}

\begin{theorem}[Eigenvectors of a Symmetric Matrix]
  Let $A$ be a symmetric matrix. Then any two eigenvectors of $A$ from different eigenspaces are
orthogonal.
\end{theorem}

\begin{theorem}[Symmetric Matrices and Orth. Diag.]
  A square matrix $A$ is orthogonally diagonalizable if and only if $A$ is a symmetric matrix.
\end{theorem}

\begin{theorem}[Spectral Theorem For Symetric Matrices]
  Let $A_{n\times n}$ be symetric. Then the following properties hold.
  \begin{itemize*}
    \item $A$ has $n$ real eigenvalues (counting multiplicities).
    \item Then dimension of the eigenspace for each eigenvalue $\lambda$ is equal to the multiplicity of $\lambda$.
    \item The eigenvectors of $A$ corresponding to different eigenvalues are orthogonal.
    \item $A$ is orthogonally diagonalizable.
  \end{itemize*}
\end{theorem}

\begin{theorem}[Principal Axis Theorem]
  Let $A_{N\times N}$ be symetric. Then there is an orthogonal change of variable $\vec{x}=P\vec{y}$ that transform the quadratic form $\vec{x}^TA\vec{x}$ to $\vec{y}^TA\vec{y}$ whith no cross terms(Ex. ofcross terms is $x_1x_2$). 
  The columns of $P$ are the eiganvectors of $A$.
\end{theorem}

\begin{theorem}[Eiganvalue of Definites Quadratic]
  The quadratic form is:
  \begin{itemize*}
    \item Positive definite if and only if $\forall i\lambda_i>0$.
    \item Negative definite if and only if $\forall i\lambda_i<0$.
    \item Indefinite if and only if $A$ as positive and negative eiganvalues.
  \end{itemize*}
\end{theorem}

\subsection{Fact about the Orthogonal Complements}
\begin{itemize*}
  \item $\vec{0}\in W^\perp$ since $\vec{0}\cdot\vec{w}=0$
  \item If $W\in W^\perp, c\in \mathbb{R}$ then $cW\in W^\perp$
  \item If $\vec{w}_1,\vec{w}_2\in W^\perp$ so $(\vec{w}_1+\vec{w}_2)\cdot\vec{x}=\vec{w}_1\cdot\vec{x}+\vec{w}_2\cdot\vec{x}\in W$
\end{itemize*}

\subsection{Gram-Schmit Process(G-S)}
The Gram-Schmit Process is an algorithme producing an orthogonal basis.\\
Start whith $\{\vec{x}_1,...,\vec{x}_n\}$ basis for noozero subspace w of $\mathbb{R}^n$ define:\\
$\vec{v}_1=\vec{x}_1$\\
$\vec{v}_2=\vec{x}_2 -\left(\frac{\vec{x}_2\cdot\vec{v}_1}{\vec{v}_1\cdot\vec{v}_1}\right)\vec{v}_1$\\
\vdots\\
$\vec{v}_n=\vec{x}_n -\left(\frac{\vec{x}_n\cdot\vec{v}_1}{\vec{v}_1\cdot\vec{v}_1}\right)\vec{v}_1- ... -\left(\frac{\vec{x}_n\cdot\vec{v}_{n-1}}{\vec{v}_{n-1}\cdot\vec{v}_{n-1}}\right)\vec{v}_{n-1}$\\


\subsection{Finding the Matrix of a Quadratic Form}
$Q(\vec{x})=Ax^2_1+Bx_1x_2+Cx^2_2 \Leftrightarrow
\begin{bmatrix}
  A & \frac{B}{2} \\
  \frac{B}{2} & C \\
\end{bmatrix}
$

\subsection{Quadratic Form Terminology}
A quadratic for $Q(\vec{x})$ is:
\begin{itemize*}
  \item Positive Definite If $Q(\vec{x})>0,\forall\vec{x}\ne\vec{0}$
  \item Positive Semidefinite If $Q(\vec{x})\ge 0$
  \item Negative Definite If $Q(\vec{x})<0,\forall\vec{x}\ne\vec{0}$
  \item Negative Semidefinite If $Q(\vec{x})\le 0$
\end{itemize*}


\end{multicols}
\end{document}